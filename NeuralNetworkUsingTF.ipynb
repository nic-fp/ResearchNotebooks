{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkUsingTF.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nic-fp/ResearchNotebooks/blob/master/NeuralNetworkUsingTF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjFYNyoHoSPU",
        "colab_type": "code",
        "outputId": "df4c2a16-ee54-4b61-a47f-2b9a281f6683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "inputSize = 28*28\n",
        "neuronsInHiddenLayer1 = 300\n",
        "neuronsInHiddenLayer2 = 200\n",
        "outputSize = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32,shape=[None,inputSize],name=\"input\")\n",
        "Y = tf.placeholder(tf.int32, shape =[None], name= \"Y\")\n",
        "\n",
        "def neuronLayer(X,numberNeurons,name,activation=None):\n",
        "  with tf.name_scope(\"neurons\"):\n",
        "    inputs = int(X.get_shape()[1])\n",
        "    standardDev = 2/ np.sqrt(inputs+numberNeurons)\n",
        "    init = tf.truncated_normal(shape=[inputs,numberNeurons],stddev=standardDev)\n",
        "    W = tf.Variable(init,name=\"weight\")\n",
        "    B = tf.zeros(shape=[numberNeurons])\n",
        "    Z = tf.matmul(X,W) +B\n",
        "    if activation is not None:\n",
        "      return activation(Z)\n",
        "    else:\n",
        "      return Z\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "  hiddenLayer1 = neuronLayer(X,neuronsInHiddenLayer1,name=\"layer1\", activation = tf.nn.relu)\n",
        "  hiddenLayer2 = neuronLayer(hiddenLayer1, neuronsInHiddenLayer2, name =\"layer2\", activation = tf.nn.relu)\n",
        "  logits = neuronLayer(hiddenLayer2,outputSize,name = \"logits\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "  xenthropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits= logits)\n",
        "  loss = tf.reduce_mean(xenthropy)\n",
        "\n",
        "learning_rate = .1\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  trainingOperation = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "  correct = tf.nn.in_top_k(logits,Y,1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
        "\n",
        "epochs = 15\n",
        "batchSize = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(epochs):\n",
        "    for iteration in range(mnist.train.num_examples//batchSize):\n",
        "      X_batch,Y_batch = mnist.train.next_batch(batchSize)\n",
        "      sess.run(trainingOperation,feed_dict= {X:X_batch,Y:Y_batch})\n",
        "    trainingAccuracy = accuracy.eval(feed_dict={X:X_batch,Y:Y_batch})\n",
        "    accuracyValidity = accuracy.eval(feed_dict= {X:mnist.validation.images,Y: mnist.validation.labels})\n",
        "    print(epoch,\"Training Accuracy:\", trainingAccuracy,\"Test Accuracy:\", accuracyValidity)\n",
        "  save_path = saver.save(sess,\"./my_model_final.ckpt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-f55ff7e98882>:48: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "0 Training Accuracy: 1.0 Test Accuracy: 0.9584\n",
            "1 Training Accuracy: 1.0 Test Accuracy: 0.9698\n",
            "2 Training Accuracy: 1.0 Test Accuracy: 0.9746\n",
            "3 Training Accuracy: 1.0 Test Accuracy: 0.9788\n",
            "4 Training Accuracy: 1.0 Test Accuracy: 0.9798\n",
            "5 Training Accuracy: 1.0 Test Accuracy: 0.9788\n",
            "6 Training Accuracy: 1.0 Test Accuracy: 0.98\n",
            "7 Training Accuracy: 1.0 Test Accuracy: 0.981\n",
            "8 Training Accuracy: 1.0 Test Accuracy: 0.9818\n",
            "9 Training Accuracy: 1.0 Test Accuracy: 0.981\n",
            "10 Training Accuracy: 1.0 Test Accuracy: 0.9816\n",
            "11 Training Accuracy: 1.0 Test Accuracy: 0.9824\n",
            "12 Training Accuracy: 1.0 Test Accuracy: 0.9824\n",
            "13 Training Accuracy: 1.0 Test Accuracy: 0.982\n",
            "14 Training Accuracy: 1.0 Test Accuracy: 0.983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjyJUS542Ut1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwhnUkOPN2xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGpBW8ULR0yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRIjJ4fcR0ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}