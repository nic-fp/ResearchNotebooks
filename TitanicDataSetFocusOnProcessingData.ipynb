{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TitanicDataSetFocusOnProcessingData.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nic-fp/ResearchNotebooks/blob/master/TitanicDataSetFocusOnProcessingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6qb1EuxfYjw",
        "colab_type": "code",
        "outputId": "ac5fe1da-9ba1-49b5-f2c3-53bc2fe6ad5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import functools\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "#importing data\n",
        "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
        "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
        "\n",
        "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
        "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\n",
        "\n",
        "LABEL_COLUMN = \"survived\"\n",
        "LABELS = [0,1]\n",
        "#creates a csv data set from link\n",
        "def get_dataset(file_path, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file_path,\n",
        "      batch_size = 10,\n",
        "      label_name= LABEL_COLUMN,\n",
        "      na_value= \"?\",\n",
        "      num_epochs =1, \n",
        "      ignore_errors = True,\n",
        "      **kwargs)\n",
        "  return dataset\n",
        "\n",
        "unpacked_test_data= get_dataset(test_file_path)\n",
        "unpacked_train_data = get_dataset(train_file_path)\n",
        "\n",
        "\n",
        "\n",
        "COLUMN_NAMES =['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone']\n",
        "\n",
        "\"\"\"SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']\n",
        "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
        "temp_dataset = get_dataset(train_file_path, \n",
        "                           select_columns=SELECT_COLUMNS,\n",
        "                           column_defaults = DEFAULTS)\"\"\"\n",
        "                            #Might need this instead\n",
        "\n",
        "\n",
        "temp_dataset = get_dataset(train_file_path, column_names= COLUMN_NAMES)\n",
        "\n",
        "#class to put all numeric data into a single column\n",
        "class PackedNumericFeatures(object):\n",
        "  def __init__(self, names):\n",
        "    self.names = names\n",
        "\n",
        "  def __call__(self, features, labels):\n",
        "    numeric_freatures = [features.pop(name) for name in self.names]\n",
        "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_freatures]\n",
        "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
        "    features['numeric'] = numeric_features\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "NUMERIC_FEATURES= ['age','n_siblings_spouses','parch', 'fare']\n",
        "packed_train_data = unpacked_train_data.map(PackedNumericFeatures(NUMERIC_FEATURES))\n",
        "packed_test_data = unpacked_test_data.map(PackedNumericFeatures(NUMERIC_FEATURES))\n",
        "\n",
        "#normalize data\n",
        "data_description = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
        "MEAN = np.array(data_description.T['mean'])\n",
        "STD = np.array(data_description.T['std'])\n",
        "\n",
        "def normalizer(data,mean,std):\n",
        "  return (data-mean)/std\n",
        "\n",
        "normalized = functools.partial(normalizer, mean= MEAN, std=STD)\n",
        "numeric_column = tf.feature_column.numeric_column('numeric',normalizer_fn= normalized,shape=[len(NUMERIC_FEATURES)])\n",
        "numeric_columns =  [numeric_column]\n",
        "\n",
        "numeric_layer = tf.keras.layers.DenseFeatures(feature_columns=numeric_columns)\n",
        "\n",
        "\n",
        "#Catagorical data\n",
        "\n",
        "CATEGORIES = {\n",
        "    'sex': ['male', 'female'],\n",
        "    'class' : ['First', 'Second', 'Third'],\n",
        "    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
        "    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n",
        "    'alone' : ['y', 'n']\n",
        "    }\n",
        "\n",
        "categorical_list = []\n",
        "\n",
        "for feature, term in CATEGORIES.items():\n",
        "  categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(key=feature,vocabulary_list=term)\n",
        "  categorical_list.append(tf.feature_column.indicator_column(categorical_column))\n",
        "\n",
        "categorical_layer = tf.keras.layers.DenseFeatures(feature_columns=categorical_list)\n",
        "\n",
        "combined_layer = tf.keras.layers.DenseFeatures(feature_columns= numeric_columns+categorical_list)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             combined_layer, \n",
        "                             tf.keras.layers.Dense(300, activation = 'relu'), \n",
        "                             tf.keras.layers.Dense(64,activation = 'sigmoid'),\n",
        "                             tf.keras.layers.Dense(1,activation= 'sigmoid')\n",
        "                             ])\n",
        "model.compile(loss='binary_crossentropy', optimizer ='adam', metrics= ['accuracy'])\n",
        "\n",
        "training_data = packed_train_data.shuffle(500)\n",
        "testing_data = packed_test_data\n",
        "\n",
        "\n",
        "\n",
        "model.fit(training_data,epochs= 30)\n",
        "\n",
        "test_loss,accuracy = model.evaluate(testing_data)\n",
        "\"\"\"print('\\n\\nTest Loss {}, Test Accuracy {}').format(test_loss,accuracy)\n",
        "\n",
        "predictions = model.predict(testing_data)\n",
        "\n",
        "# Show some results\n",
        "for prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
        "  print(\"Predicted survival: {:.2%}\".format(prediction[0]),\n",
        "        \" | Actual outcome: \",\n",
        "        (\"SURVIVED\" if bool(survived) else \"DIED\"))\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.5679 - accuracy: 0.6810\n",
            "Epoch 2/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.4405 - accuracy: 0.8134\n",
            "Epoch 3/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.4215 - accuracy: 0.8230\n",
            "Epoch 4/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.4169 - accuracy: 0.8246\n",
            "Epoch 5/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3914 - accuracy: 0.8373\n",
            "Epoch 6/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3949 - accuracy: 0.8389\n",
            "Epoch 7/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3890 - accuracy: 0.8341\n",
            "Epoch 8/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3802 - accuracy: 0.8421\n",
            "Epoch 9/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3787 - accuracy: 0.8325\n",
            "Epoch 10/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3703 - accuracy: 0.8485\n",
            "Epoch 11/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3688 - accuracy: 0.8469\n",
            "Epoch 12/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3646 - accuracy: 0.8485\n",
            "Epoch 13/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3575 - accuracy: 0.8517\n",
            "Epoch 14/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3559 - accuracy: 0.8517\n",
            "Epoch 15/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3489 - accuracy: 0.8517\n",
            "Epoch 16/30\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.3461 - accuracy: 0.8581\n",
            "Epoch 17/30\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8549\n",
            "Epoch 18/30\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.3418 - accuracy: 0.8612\n",
            "Epoch 19/30\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.3366 - accuracy: 0.8549\n",
            "Epoch 20/30\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8581\n",
            "Epoch 21/30\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.3423 - accuracy: 0.8469\n",
            "Epoch 22/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3300 - accuracy: 0.8549\n",
            "Epoch 23/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3252 - accuracy: 0.8612\n",
            "Epoch 24/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3320 - accuracy: 0.8628\n",
            "Epoch 25/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3233 - accuracy: 0.8596\n",
            "Epoch 26/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3193 - accuracy: 0.8628\n",
            "Epoch 27/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.8612\n",
            "Epoch 28/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3218 - accuracy: 0.8692\n",
            "Epoch 29/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3130 - accuracy: 0.8692\n",
            "Epoch 30/30\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.8660\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4272 - accuracy: 0.8447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'print(\\'\\n\\nTest Loss {}, Test Accuracy {}\\').format(test_loss,accuracy)\\n\\npredictions = model.predict(testing_data)\\n\\n# Show some results\\nfor prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\\n  print(\"Predicted survival: {:.2%}\".format(prediction[0]),\\n        \" | Actual outcome: \",\\n        (\"SURVIVED\" if bool(survived) else \"DIED\"))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDcRnvuPAgrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}